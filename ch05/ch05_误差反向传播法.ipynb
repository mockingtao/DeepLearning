{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bace346b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T03:54:54.098071Z",
     "start_time": "2022-12-27T03:54:52.872446Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import pickle\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell \n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66462d8",
   "metadata": {},
   "source": [
    "本章我们将学习一个能够高效计算权重参数的梯度的方法——误差反向传播法。\n",
    "\n",
    "要正确理解误差反向传播法，我个人认为有两种方法：\n",
    "- 一种是基于数学式；\n",
    "- 另一种是基于计算图（computational graph）。\n",
    "\n",
    "前者是比较常见的方法，机器学习相关的图书中多数都是以数学式为中心展开论述的。因为这种方法严密且简洁，所以确实非常合理，但如果一上来就围绕数学式进行探讨，会忽略一些根本的东西，止步于式子的罗列。因此，本章希望大家通过计算图，直观地理解误差反向传播法。然后，再结合实际的代码加深理解，相信大家一定会有种“原来如此！”的感觉。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5562f321",
   "metadata": {},
   "source": [
    "# 计算图"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242ac62b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-17T09:19:48.256353Z",
     "start_time": "2022-12-17T09:19:48.252790Z"
    }
   },
   "source": [
    "<img src=\"img/5_3.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a07735",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-17T09:18:48.118460Z",
     "start_time": "2022-12-17T09:18:48.114535Z"
    }
   },
   "source": [
    "综上，用计算图解题的情况下，需要按如下流程进行。\n",
    "\n",
    "1. 构建计算图。\n",
    "2. 在计算图上，从左向右进行计算。\n",
    "\n",
    "- 这里的第2歩“从左向右进行计算”是一种正方向上的传播，简称为正向传播（forward propagation）。正向传播是从计算图出发点到结束点的传播。\n",
    "- 既然有正向传播这个名称，当然也可以考虑反向（从图上看的话，就是从右向左）的传播。实际上，这种传播称为反向传播（backward propagation）。反向传播将在接下来的导数计算中发挥重要作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207b93b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-17T09:28:51.203362Z",
     "start_time": "2022-12-17T09:28:51.199792Z"
    }
   },
   "source": [
    "计算图的特征是可以通过传递**“局部计算”**获得最终结果。“局部”这个词的意思是“与自己相关的某个小范围”。局部计算是指，无论全局发生了什么，都能只根据与自己相关的信息输出接下来的结果。\n",
    "\n",
    "虽然局部计算非常简单，但是通过传递它的计算结果，可以获得全局的复杂计算的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfb90db",
   "metadata": {},
   "source": [
    "**为何用计算图解题？**\n",
    "\n",
    "- 一个优点就在于前面所说的局部计算。无论全局是多么复杂的计算，都可以通过局部计算使各个节点致力于简单的计算，从而简化问题。\n",
    "- 另一个优点是，利用计算图可以将中间的计算结果全部保存起来（比如，计算进行到2个苹果时的金额是200日元、加上消费税之前的金额650日元等）。\n",
    "- **实际上，使用计算图最大的原因是，可以通过反向传播高效计算导数。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fface8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T23:50:22.880768Z",
     "start_time": "2022-12-26T23:50:22.877788Z"
    }
   },
   "source": [
    "<img src=\"img/5_5.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "如图5-5所示，反向传播使用与正方向相反的箭头（粗线）表示。反向传播传递“局部导数”，将导数的值写在箭头的下方。\n",
    "\n",
    "在这个例子中，反向传播从右向左传递导数的值（1 → 1.1 → 2.2）。从这个结果中可知，“支付金额关于苹果的价格的导数”的值是2.2。\n",
    "\n",
    "这意味着，如果苹果的价格上涨1日元，最终的支付金额会增加2.2日元（严格地讲，如果苹果的价格增加某个微小值，\n",
    "则最终的支付金额将增加那个微小值的2.2倍）。\n",
    "\n",
    "这里只求了关于苹果的价格的导数，不过“支付金额关于消费税的导数”“支付金额关于苹果的个数的导数”等也都可以用同样的方式算出来。并且，计算中途求得的导数的结果（中间传递的导数）可以被共享，从而可以高效地计算多个导数。\n",
    "\n",
    "综上，计算图的优点是，可以通过正向传播和反向传播高效地计算各个变量的导数值。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0be47bd",
   "metadata": {},
   "source": [
    "# 链式法则\n",
    "\n",
    "链式法则是关于复合函数的导数的性质，定义如下：\n",
    "\n",
    "> 如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示。\n",
    "\n",
    "从下面图例可以简单理解：链式法则就是利用复合函数的乘积定义，来一层一层向上游求导。\n",
    "\n",
    "<img src=\"img/5_5_2.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "<img src=\"img/5_7.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4941e62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T07:08:23.486777Z",
     "start_time": "2022-10-21T07:08:23.483202Z"
    }
   },
   "source": [
    "# 反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b7ccca",
   "metadata": {},
   "source": [
    "## 加法节点的反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef178784",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T00:14:01.432384Z",
     "start_time": "2022-12-27T00:14:01.429144Z"
    }
   },
   "source": [
    "<img src=\"img/5_9.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "因为加法节点的反向传播只乘以1，所以输入的值会原封不动地流向下一个节点。\n",
    "\n",
    "**简单理解就是：遇到加法直接把“右侧的导出”传递到“左侧”即可。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cda2d2",
   "metadata": {},
   "source": [
    "## 乘法节点的反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9a3ab5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T00:17:38.312747Z",
     "start_time": "2022-12-27T00:17:38.309589Z"
    }
   },
   "source": [
    "<img src=\"img/5_12.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "**乘法的反向传播会将上游的值乘以正向传播时的输入信号的“翻转值”后传递给下游。**\n",
    "\n",
    "翻转值表示一种翻转关系，如图5-12所示，正向传播时信号是x的话，反向传播时则是y；正向传播时信号是y的话，反向传播时则是x。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd28b81",
   "metadata": {},
   "source": [
    "# 简单层的实现\n",
    "\n",
    "这里对以上两个节点：“加法节点”和“乘法节点”进行简单的代码实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ab61a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T07:28:02.790140Z",
     "start_time": "2022-10-21T07:28:02.785377Z"
    }
   },
   "source": [
    "## 乘法层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c219eb7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T03:54:54.104002Z",
     "start_time": "2022-12-27T03:54:54.100368Z"
    }
   },
   "outputs": [],
   "source": [
    "# 乘法层的实现\n",
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y # 翻转x和y\n",
    "        dy = dout * self.x\n",
    "        return dx, dy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0637d17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T03:54:54.136629Z",
     "start_time": "2022-12-27T03:54:54.107095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "220.00000000000003\n",
      "1.1 200\n",
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "# 乘法层实例测试\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer() # 评估总价计算层\n",
    "mul_tax_layer = MulLayer() # 总价税收计算层\n",
    "# 每一层都会生成一个类对象，其中之一的原因是为了保存这一层的x和y值，后面会用来进行”反向传播“（求导）\n",
    "\n",
    "# forward\n",
    "# cc：在这一步除了正向传播计算之外，通过类变量的方式存储了每一层的x 和 y，这样在执行下面backward的时候，就可以计算出对应“导数”结果；\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "print(apple_price)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "print(price) # 220\n",
    "\n",
    "# 上面两个print结果好理解：\n",
    "# 1. 2个苹果总价格\n",
    "# 2. 苹果总价 + 税收 之后的总价格\n",
    "\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "print(dapple_price, dtax)\n",
    "\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "print(dapple, dapple_num, dtax) # 2.2 110 200\n",
    "\n",
    "# 结果如下图"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd438bf",
   "metadata": {},
   "source": [
    "结果如图：\n",
    "\n",
    "\n",
    "<img src=\"img/5_13.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaafcf72",
   "metadata": {},
   "source": [
    "## 加法层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "145cd5d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T03:54:54.142822Z",
     "start_time": "2022-12-27T03:54:54.139455Z"
    }
   },
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b128a3ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T03:54:54.150928Z",
     "start_time": "2022-12-27T03:54:54.145147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
     ]
    }
   ],
   "source": [
    "# 加法层 + 乘法层 实例测试\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num) #(1)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num) #(2)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price) #(3)\n",
    "price = mul_tax_layer.forward(all_price, tax) #(4)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice) #(4)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price) #(3)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price) #(2)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price) #(1)\n",
    "\n",
    "print(price) # 715\n",
    "print(dapple_num, dapple, dorange, dorange_num, dtax) # 110 2.2 3.3 165 650\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd87c02e",
   "metadata": {},
   "source": [
    "# 激活函数层的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856a8a41",
   "metadata": {},
   "source": [
    "现在，我们将计算图的思路应用到神经网络中。这里，我们把构成神经网络的层实现为一个类。先来实现激活函数的ReLU层和Sigmoid层。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc748b77",
   "metadata": {},
   "source": [
    "## ReLU层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f197d7b",
   "metadata": {},
   "source": [
    "<img src=\"img/f_5_7.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "<img src=\"img/5_18.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebf0cf1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T03:54:54.157431Z",
     "start_time": "2022-12-27T03:54:54.153125Z"
    }
   },
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "         \n",
    "    # x 传进来的是一个numpy数组；\n",
    "    # mask 就成了x中小于等于0为True的标记位；\n",
    "    # 然后让out = x之后，对out为mask标记位为True置为0，那么其他为False 为原值\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "    \n",
    "    # dout同理，mask标记位为True的，dout改为0，其他不变\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        return dx\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42146a38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T03:54:54.164537Z",
     "start_time": "2022-12-27T03:54:54.160134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n",
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "# mask标记位的代码解释\n",
    "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
    "print(x)\n",
    "mask = (x <= 0)\n",
    "print(mask)\n",
    "\n",
    "# 上文代码中，会将为True的地方（<=0）设置为0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd236890",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T03:54:54.178273Z",
     "start_time": "2022-12-27T03:54:54.169281Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 3.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Relu 测试\n",
    "relu_test = Relu()\n",
    "a = relu_test.forward(x)\n",
    "a\n",
    "dout = np.array([[1, 8], [1, 1]])\n",
    "b = relu_test.backward(dout)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cb796f",
   "metadata": {},
   "source": [
    "## Sigmoid 层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb9e496",
   "metadata": {},
   "source": [
    "<img src=\"img/5_19.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<img src=\"img/5_20.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "这个值只根据正向传播时的输入x和输出y就可以算出来。因此，图5-20的计算图可以画成图5-21的集约化的“sigmoid”节点。\n",
    "<img src=\"img/5_21.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "经过公式推导之后，得到这样的结果，这样就更方便代码实现了。\n",
    "<img src=\"img/5_22.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b750e25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T03:54:54.186782Z",
     "start_time": "2022-12-27T03:54:54.182937Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54c5d2ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T03:54:54.194271Z",
     "start_time": "2022-12-27T03:54:54.189299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.73105858 0.37754067]\n",
      " [0.11920292 0.95257413]]\n",
      "[[0.19661193 0.23500371]\n",
      " [0.10499359 0.04517666]]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "sig_test = Sigmoid()\n",
    "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
    "forward = sig_test.forward(x)\n",
    "print(forward)\n",
    "\n",
    "dout = np.array([[1, 1], [1, 1]])\n",
    "backward = sig_test.backward(dout)\n",
    "print(backward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4166c9b4",
   "metadata": {},
   "source": [
    "# Affine/Softmax层的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d3ea10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T10:02:22.712103Z",
     "start_time": "2022-10-21T10:02:22.707831Z"
    }
   },
   "source": [
    "## Affine 层\n",
    "\n",
    "神经网络的正向传播中，为了计算加权信号的总和，使用了矩阵的乘积运算。\n",
    "\n",
    "这个乘积运算在几何学领域被称为“仿射变换”。因此，这里将进行仿射变换的处理实现为“Affine层”。关于仿射变换的几何数学解释可以在链接查看：https://www.matongxue.com/madocs/244/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4570feaa",
   "metadata": {},
   "source": [
    "- 将乘积运算用“dot”节点表示的话，则np.dot(X, W) + B的运算可用图5-24所示的计算图表示出来。\n",
    "- 另外，在各个变量的上方标记了它们的形状（比如，计算图上显示了X的形状为(2,)，X·W的形状为(3,)等）。\n",
    "- 现在这里的各个节点传播的是矩阵，之前我们看到的都是标量\n",
    "\n",
    "\n",
    "<img src=\"img/5_24.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "<img src=\"img/5_25.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<img src=\"img/5_27.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d84c4146",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T03:54:54.208819Z",
     "start_time": "2022-12-27T03:54:54.196552Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0],\n",
       "       [10, 10, 10]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3],\n",
       "       [11, 12, 13]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([5, 7, 9])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 和偏置B有关一点知识\n",
    "X_dot_W = np.array([[0, 0, 0], [10, 10, 10]])\n",
    "B = np.array([1, 2, 3])   \n",
    "X_dot_W\n",
    "X_dot_W + B\n",
    "\n",
    "# 正向传播时，偏置被加到X·W的各个数据上。cc：这个很好理解。\n",
    "\n",
    "# 反向传播时，各个数据的反向传播的值需要汇总为偏置的元素。cc： 这个就不好理解了。\n",
    "dY = np.array([[1, 2, 3,], [4, 5, 6]])\n",
    "dY\n",
    "\n",
    "dB = np.sum(dY, axis=0)\n",
    "dB\n",
    " \n",
    "# 可以参考在5-27的第3个公式，但是还是没有理解。先过~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52984876",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T03:54:54.216253Z",
     "start_time": "2022-12-27T03:54:54.210910Z"
    }
   },
   "outputs": [],
   "source": [
    "# 批版本的Affine层\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        return dx\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af022805",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T10:02:33.562163Z",
     "start_time": "2022-10-21T10:02:33.558200Z"
    }
   },
   "source": [
    "## Softmax-with-Loss 层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8d6d31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T03:17:41.817105Z",
     "start_time": "2022-12-27T03:17:41.814001Z"
    }
   },
   "source": [
    "在第3章讲到，softmax函数会将输入值正规化之后再输出。\n",
    "\n",
    "<img src=\"img/5_28.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "这里把第3章的神经网络拿出来做个类比：（上下两图刚好都是3层：Affine+ReLU一组为一层）\n",
    "\n",
    "- x，w，b的运算为Affine层，就是通过dot做了”仿射变换“\n",
    "- h()对应这里的ReLU层（如果激活函数用的ReLU，那也可以是Sigmoid函数）\n",
    "<img src=\"img/3_18_copy.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "\n",
    "神经网络中进行的处理有推理（inference）和学习两个阶段。\n",
    "- 神经网络的推理通常不使用 Softmax层。\n",
    "    比如，用图 5-28的网络进行推理时，会将最后一个 Affine层的输出作为识别结果。神经网络中未被正规化的输出结果（图 5-28中 Softmax层前面的 Affine层的输出）有时被称为“得分”。也就是说，当神经网络的推理只需要给出一个答案的情况下，因为此时只对得分最大值感兴趣，所以不需要 Softmax层。\n",
    "- 不过，神经网络的学习阶段则需要 Softmax层。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a768e",
   "metadata": {},
   "source": [
    "考虑到这里也包含作为损失函数的交叉熵误差（cross entropy error），所以称为“Softmax-with-Loss层”\n",
    "\n",
    "<img src=\"img/5_29.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<img src=\"img/5_30.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "- softmax函数记为Softmax层，交叉熵误差记为Cross Entropy Error层。\n",
    "- 这里假设要进行3类分类，从前面的层接收3个输入（得分）。如图5-30所示，Softmax层将输入（a1, a2, a3）正规化，输出（y1, y2, y3）。\n",
    "- Cross Entropy Error层接收Softmax的输出（y1, y2, y3）和标签（t1, t2, t3），从这些数据中输出损失L。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f440c142",
   "metadata": {},
   "source": [
    "**重点：**\n",
    "\n",
    "图5-30中要注意的是反向传播的结果。Softmax层的反向传播得到了（y1 − t1, y2 − t2, y3 − t3）这样“漂亮”的结果（公式这里就不推导了）。由于（y1, y2, y3）是Softmax层的输出，（t1, t2, t3）是监督数据，所以（y1 − t1, y2 − t2, y3 − t3）是Softmax层的输出和标签的差分。**神经网络的反向传播会把这个差分表示的误差传递给前面的层，这是神经网络学习中的重要性质。**\n",
    "\n",
    "神经网络学习的目的就是通过调整权重参数，使神经网络的输出（Softmax的输出）接近标签。因此，必须将神经网络的输出与教师标签的误差高效地传递给前面的层。刚刚的（y1 − t1, y2 − t2, y3 − t3）正是Softmax层的输出与标签的差，直截了当地表示了当前神经网络的输出与标签的误差。\n",
    "\n",
    "使用交叉熵误差作为 softmax函数的损失函数后，反向传播得到（y1 − t1, y2 − t2, y3 − t3）这样“漂亮”的结果。实际上，这样“漂亮”的结果并不是偶然的，而是为了得到这样的结果，特意设计了交叉熵误差函数。回归问题中输出层使用“恒等函数”，损失函数使用“平方和误差”，也是出于同样的理由（3.5节）。也就是说，使用“平方和误差”作为“恒等函数”的损失函数，反向传播才能得到（y1 − t1, y2 − t2, y3 − t3）这样“漂亮”的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d26f594",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T03:54:54.224057Z",
     "start_time": "2022-12-27T03:54:54.218822Z"
    }
   },
   "outputs": [],
   "source": [
    "# 代码实现\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 损失\n",
    "        self.y = None # softmax的输出\n",
    "        self.t = None # 监督数据（one-hot vector）\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1dc3f2",
   "metadata": {},
   "source": [
    "这里考虑一个具体的例子，比如标签是（0, 1, 0），Softmax层的输出是(0.3, 0.2, 0.5)的情形。因为正确解标签处的概率是0.2（20%），这个时候的神经网络未能进行正确的识别。此时，Softmax层的反向传播传递的是(0.3, −0.8, 0.5)这样一个大的误差。因为这个大的误差会向前面的层传播，所以Softmax层前面的层会从这个大的误差中学习到“大”的内容。\n",
    "\n",
    "再举一个例子，比如标签是(0, 1, 0)，Softmax层的输出是(0.01, 0.99, 0)的情形（这个神经网络识别得相当准确）。此时Softmax层的反向传播传递的是(0.01, −0.01, 0)这样一个小的误差。这个小的误差也会向前面的层传播，因为误差很小，所以Softmax层前面的层学到的内容也很“小”。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184cda8a",
   "metadata": {},
   "source": [
    "# 误差反向传播法的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabffb10",
   "metadata": {},
   "source": [
    "在进行具体的实现之前，我们再来确认一下神经网络学习的全貌图。神经网络学习的步骤如下所示。\n",
    "\n",
    "**前提**\n",
    "\n",
    "神经网络中有合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为学习。神经网络的学习分为下面4个步骤。\n",
    "\n",
    "- 步骤1（mini-batch）\n",
    "    从训练数据中随机选择一部分数据。\n",
    "- 步骤2（计算梯度）\n",
    "    计算损失函数关于各个权重参数的梯度。\n",
    "- 步骤3（更新参数）\n",
    "    将权重参数沿梯度方向进行微小的更新。\n",
    "- 步骤4（重复）\n",
    "    重复步骤1、步骤2、步骤3。\n",
    "    \n",
    "误差反向传播法会在步骤2中出现。上一章中，我们利用数值微分求得了这个梯度。数值微分虽然实现简单，但是计算要耗费较多的时间。和需要花费较多时间的数值微分不同，误差反向传播法可以快速高效地计算梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fb2e1a",
   "metadata": {},
   "source": [
    "## 误差反向传播法的神经网络的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b7e345c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T12:06:34.655083Z",
     "start_time": "2022-12-27T12:06:33.438287Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 初始化参数，是一个字典。\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        # 生成层\n",
    "        self.layers = OrderedDict() # cc：有序字典：“有序”是指它可以记住向字典里添加元素的顺序。\n",
    "        # cc：以layers['Affine1']、layers['ReLu1']、layers['Affine2']的形式，通过有序字典保存各个层\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1']) # 把Affine 对象存到layers里面\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x) # cc：求出每一层的output，但是只返回最后一次的结果。中间层的结果不返回，但是会保存其类变量，供反向传播使用；\n",
    "        return x\n",
    "\n",
    "    # x:输入数据, t:监督数据\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "            \n",
    "    # x:输入数据, t:监督数据\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse() # 这里对layers做了反序，则 开始反向传播，调取每个对象的backward方法\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        # 设定\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430c17b8",
   "metadata": {},
   "source": [
    "像这样通过将神经网络的组成元素以层的方式实现，可以轻松地构建神经网络。这个用层进行模块化的实现具有很大优点。因为想另外构建一个神经网络（比如5层、10层、20层……的大的神经网络）时，只需像组装乐高积木那样添加必要的层就可以了。之后，通过各个层内部实现的正向传播和反向传播，就可以正确计算进行识别处理或学习所需的梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b89dd4",
   "metadata": {},
   "source": [
    "## 误差反向传播法的梯度验证正确性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "331fd0dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T12:14:42.521260Z",
     "start_time": "2022-12-27T12:14:37.322996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:5.080372264776923e-10\n",
      "b1:2.8296791743684206e-09\n",
      "W2:5.2661368126302625e-09\n",
      "b2:1.4004956144508806e-07\n"
     ]
    }
   ],
   "source": [
    "# 误差反向传播法的梯度确认: 微分法 和 解析法 求差值；\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 读入数据\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label = True)\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "x_batch = x_train[:3]\n",
    "\n",
    "t_batch = t_train[:3]\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 求各个权重的绝对误差的平均值\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key + \":\" + str(diff))\n",
    "#     print('grad_numerical:',grad_numerical[key])\n",
    "#     print('grad_backprop:', grad_backprop[key])\n",
    "\n",
    "# 从这个结果可以看出，通过数值微分和误差反向传播法求出的梯度的差非常小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cb52f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T12:15:33.776451Z",
     "start_time": "2022-12-27T12:15:33.772398Z"
    }
   },
   "source": [
    "数值微分和误差反向传播法的计算结果之间的误差为 0是很少见的。这是因为计算机的计算精度有限（比如，32位浮点数）。受到数值精度的限制，刚才的误差一般不会为 0，但是如果实现正确的话，可以期待这个误差是一个接近 0的很小的值。如果这个值很大，就说明误差反向传播法的实现存在错误。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6830e551",
   "metadata": {},
   "source": [
    "## 案例手写数字：使用误差反向传播法的学习\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d0bdc04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T12:19:08.745344Z",
     "start_time": "2022-12-27T12:18:53.734905Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11775 0.1181\n",
      "0.9046666666666666 0.9081\n",
      "0.92405 0.927\n",
      "0.93465 0.9343\n",
      "0.9459 0.9445\n",
      "0.9513166666666667 0.9484\n",
      "0.9578666666666666 0.9532\n",
      "0.9617333333333333 0.9567\n",
      "0.9657 0.9602\n",
      "0.96895 0.9625\n",
      "0.9718 0.9665\n",
      "0.9732 0.9677\n",
      "0.9740833333333333 0.9668\n",
      "0.975 0.9684\n",
      "0.97795 0.9705\n",
      "0.9791333333333333 0.9702\n",
      "0.9801333333333333 0.9702\n"
     ]
    }
   ],
   "source": [
    "# 使用误差反向传播法的学习\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "# 读入数据\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 通过误差反向传播法求梯度\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 更新：每轮都会更新这4个参数值\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717f59e0",
   "metadata": {},
   "source": [
    "# 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0c3692",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T12:20:59.625369Z",
     "start_time": "2022-12-27T12:20:59.622023Z"
    }
   },
   "source": [
    "本章所学的内容\n",
    "- 通过使用计算图，可以直观地把握计算过程。\n",
    "- 计算图的节点是由局部计算构成的。局部计算构成全局计算。\n",
    "- 计算图的正向传播进行一般的计算。通过计算图的反向传播，可以计算各个节点的导数。\n",
    "- 通过将神经网络的组成元素实现为层，可以高效地计算梯度（反向传播法）。\n",
    "- 通过比较数值微分和误差反向传播法的结果，可以确认误差反向传播法的实现是否正确（梯度确认）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6af642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a58b0e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba251e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "209.46px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
