{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bace346b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-17T09:14:40.163282Z",
     "start_time": "2022-12-17T09:14:40.160527Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import pickle\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell \n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66462d8",
   "metadata": {},
   "source": [
    "本章我们将学习一个能够高效计算权重参数的梯度的方法——误差反向传播法。\n",
    "\n",
    "要正确理解误差反向传播法，我个人认为有两种方法：\n",
    "- 一种是基于数学式；\n",
    "- 另一种是基于计算图（computational graph）。\n",
    "\n",
    "前者是比较常见的方法，机器学习相关的图书中多数都是以数学式为中心展开论述的。因为这种方法严密且简洁，所以确实非常合理，但如果一上来就围绕数学式进行探讨，会忽略一些根本的东西，止步于式子的罗列。因此，本章希望大家通过计算图，直观地理解误差反向传播法。然后，再结合实际的代码加深理解，相信大家一定会有种“原来如此！”的感觉。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5562f321",
   "metadata": {},
   "source": [
    "# 计算图"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242ac62b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-17T09:19:48.256353Z",
     "start_time": "2022-12-17T09:19:48.252790Z"
    }
   },
   "source": [
    "<img src=\"img/5_3.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a07735",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-17T09:18:48.118460Z",
     "start_time": "2022-12-17T09:18:48.114535Z"
    }
   },
   "source": [
    "综上，用计算图解题的情况下，需要按如下流程进行。\n",
    "\n",
    "1. 构建计算图。\n",
    "2. 在计算图上，从左向右进行计算。\n",
    "\n",
    "- 这里的第2歩“从左向右进行计算”是一种正方向上的传播，简称为正向传播（forward propagation）。正向传播是从计算图出发点到结束点的传播。\n",
    "- 既然有正向传播这个名称，当然也可以考虑反向（从图上看的话，就是从右向左）的传播。实际上，这种传播称为反向传播（backward propagation）。反向传播将在接下来的导数计算中发挥重要作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207b93b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-17T09:28:51.203362Z",
     "start_time": "2022-12-17T09:28:51.199792Z"
    }
   },
   "source": [
    "计算图的特征是可以通过传递**“局部计算”**获得最终结果。“局部”这个词的意思是“与自己相关的某个小范围”。局部计算是指，无论全局发生了什么，都能只根据与自己相关的信息输出接下来的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0be47bd",
   "metadata": {},
   "source": [
    "# 链式法则"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4941e62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T07:08:23.486777Z",
     "start_time": "2022-10-21T07:08:23.483202Z"
    }
   },
   "source": [
    "# 反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd28b81",
   "metadata": {},
   "source": [
    "# 简单层的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ab61a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T07:28:02.790140Z",
     "start_time": "2022-10-21T07:28:02.785377Z"
    }
   },
   "source": [
    "### 乘法层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c219eb7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T07:25:15.761133Z",
     "start_time": "2022-10-21T07:25:15.749463Z"
    }
   },
   "outputs": [],
   "source": [
    "# 乘法层的实现\n",
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y # 翻转x和y\n",
    "        dy = dout * self.x\n",
    "        return dx, dy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0637d17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-22T04:00:36.837007Z",
     "start_time": "2022-10-22T04:00:36.814005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "220.00000000000003\n",
      "1.1 200\n",
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "# 乘法层实例测试\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer() # 评估总价计算层\n",
    "mul_tax_layer = MulLayer() # 总价税收计算层\n",
    "\n",
    "# forward\n",
    "# cc：在这一步除了正向传播计算之外，实际还定义了每一层的x 和 y，这样在执行backward的时候，就可以计算出对应“导数”结果；\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "print(apple_price)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "print(price) # 220\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "print(dapple_price, dtax)\n",
    "\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "print(dapple, dapple_num, dtax) # 2.2 110 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaafcf72",
   "metadata": {},
   "source": [
    "### 加法层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "145cd5d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T07:28:52.084253Z",
     "start_time": "2022-10-21T07:28:52.079021Z"
    }
   },
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b128a3ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T07:31:46.411082Z",
     "start_time": "2022-10-21T07:31:46.393858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
     ]
    }
   ],
   "source": [
    "# 加法层 + 乘法层 实例测试\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num) #(1)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num) #(2)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price) #(3)\n",
    "price = mul_tax_layer.forward(all_price, tax) #(4)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice) #(4)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price) #(3)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price) #(2)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price) #(1)\n",
    "\n",
    "print(price) # 715\n",
    "print(dapple_num, dapple, dorange, dorange_num, dtax) # 110 2.2 3.3 165 650\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd87c02e",
   "metadata": {},
   "source": [
    "# 激活函数层的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc748b77",
   "metadata": {},
   "source": [
    "### ReLU层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebf0cf1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-22T04:14:05.681950Z",
     "start_time": "2022-10-22T04:14:05.663809Z"
    }
   },
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        return dx\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42146a38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T07:46:43.581313Z",
     "start_time": "2022-10-21T07:46:43.574573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n",
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "# 详细解释\n",
    "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
    "print(x)\n",
    "mask = (x <= 0)\n",
    "print(mask)\n",
    "\n",
    "# 上文代码中，会将为True的地方（<=0）设置为0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd236890",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-22T04:17:20.577774Z",
     "start_time": "2022-10-22T04:17:20.560764Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 3.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Relu 测试\n",
    "relu_test = Relu()\n",
    "a = relu_test.forward(x)\n",
    "a\n",
    "dout = np.array([[1, 1], [1, 1]])\n",
    "b = relu_test.backward(dout)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cb796f",
   "metadata": {},
   "source": [
    "### Sigmoid 层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b750e25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-22T07:11:43.344876Z",
     "start_time": "2022-10-22T07:11:43.336904Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54c5d2ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-22T07:12:26.267198Z",
     "start_time": "2022-10-22T07:12:26.258588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.73105858 0.37754067]\n",
      " [0.11920292 0.95257413]]\n",
      "[[0.19661193 0.23500371]\n",
      " [0.10499359 0.04517666]]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "sig_test = Sigmoid()\n",
    "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
    "forward = sig_test.forward(x)\n",
    "print(forward)\n",
    "\n",
    "dout = np.array([[1, 1], [1, 1]])\n",
    "backward = sig_test.backward(dout)\n",
    "print(backward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4166c9b4",
   "metadata": {},
   "source": [
    "# Affine/Softmax层的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d3ea10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T10:02:22.712103Z",
     "start_time": "2022-10-21T10:02:22.707831Z"
    }
   },
   "source": [
    "### Affine 层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d84c4146",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-22T07:24:30.370377Z",
     "start_time": "2022-10-22T07:24:30.338148Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0],\n",
       "       [10, 10, 10]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3],\n",
       "       [11, 12, 13]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([5, 7, 9])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 和偏置B有关一点知识：\n",
    "X_dot_W = np.array([[0, 0, 0], [10, 10, 10]])\n",
    "B = np.array([1, 2, 3])   \n",
    "X_dot_W\n",
    "X_dot_W + B\n",
    "\n",
    "# 正向传播时，偏置被加到X·W的各个数据上。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea3722fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-22T07:25:31.425921Z",
     "start_time": "2022-10-22T07:25:31.415276Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([5, 7, 9])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dY = np.array([[1, 2, 3,], [4, 5, 6]])\n",
    "dY\n",
    "\n",
    "dB = np.sum(dY, axis=0)\n",
    "dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52984876",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T10:00:25.066031Z",
     "start_time": "2022-10-21T10:00:25.059630Z"
    }
   },
   "outputs": [],
   "source": [
    "# 批版本的Affine层\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        return dx\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af022805",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T10:02:33.562163Z",
     "start_time": "2022-10-21T10:02:33.558200Z"
    }
   },
   "source": [
    "### Softmax-with-Loss 层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d26f594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 代码实现\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 损失\n",
    "        self.y = None # softmax的输出\n",
    "        self.t = None # 监督数据（one-hot vector）\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184cda8a",
   "metadata": {},
   "source": [
    "# 误差反向传播法的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7e345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 应误差反向传播法的神经网络的实现\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 初始化权重\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        # 生成层\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    # x:输入数据, t:监督数据\n",
    "     def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "            \n",
    "    # x:输入数据, t:监督数据\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        # 设定\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "331fd0dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-22T07:39:39.528399Z",
     "start_time": "2022-10-22T07:39:29.393405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:4.01485263996341e-10\n",
      "grad_numerical: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "grad_backprop: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "b1:2.429664222683989e-09\n",
      "grad_numerical: [-3.37947164e-03  4.88785920e-03 -6.94066149e-04 -2.29474873e-03\n",
      "  0.00000000e+00  0.00000000e+00 -2.67046518e-05 -4.33291440e-03\n",
      "  2.76217285e-03  4.73053130e-04  3.53758641e-03  4.09163211e-03\n",
      "  5.89817531e-03 -1.84379453e-03  0.00000000e+00  0.00000000e+00\n",
      " -6.21474051e-03  0.00000000e+00 -5.12573816e-03  2.15976792e-03\n",
      "  0.00000000e+00  5.80148352e-04  0.00000000e+00  6.31418314e-03\n",
      " -2.09430849e-03  1.40039111e-03 -3.27826762e-03 -4.21013644e-04\n",
      " -1.10998321e-03  3.15677806e-03  4.06703081e-03  5.79197049e-03\n",
      "  6.90318001e-03  1.88692855e-03  0.00000000e+00 -3.73503519e-03\n",
      " -4.61026186e-03  0.00000000e+00  2.72321092e-03 -5.89298915e-03\n",
      " -3.01005435e-03  0.00000000e+00  0.00000000e+00  3.30341384e-03\n",
      " -2.02439369e-03  0.00000000e+00  3.26916515e-03 -1.94692253e-03\n",
      " -3.97882310e-03 -2.05947194e-03]\n",
      "grad_backprop: [-3.37947502e-03  4.88786408e-03 -6.94066851e-04 -2.29475101e-03\n",
      "  0.00000000e+00  0.00000000e+00 -2.67046792e-05 -4.33291875e-03\n",
      "  2.76217563e-03  4.73053606e-04  3.53758996e-03  4.09163622e-03\n",
      "  5.89818119e-03 -1.84379637e-03  0.00000000e+00  0.00000000e+00\n",
      " -6.21474674e-03  0.00000000e+00 -5.12574329e-03  2.15977008e-03\n",
      "  0.00000000e+00  5.80148933e-04  0.00000000e+00  6.31418947e-03\n",
      " -2.09431059e-03  1.40039251e-03 -3.27827090e-03 -4.21014067e-04\n",
      " -1.10998435e-03  3.15678122e-03  4.06703487e-03  5.79197629e-03\n",
      "  6.90318693e-03  1.88693044e-03  0.00000000e+00 -3.73503894e-03\n",
      " -4.61026649e-03  0.00000000e+00  2.72321364e-03 -5.89299506e-03\n",
      " -3.01005737e-03  0.00000000e+00  0.00000000e+00  3.30341716e-03\n",
      " -2.02439573e-03  0.00000000e+00  3.26916843e-03 -1.94692447e-03\n",
      " -3.97882707e-03 -2.05947399e-03]\n",
      "W2:5.55285049673443e-09\n",
      "grad_numerical: [[-0.01972029  0.00218487  0.00219375  0.00219291  0.00217902  0.00219309\n",
      "   0.0021983   0.00220148  0.00219286  0.002184  ]\n",
      " [ 0.00149651  0.00279612  0.00281169  0.00280332 -0.02394666  0.00282041\n",
      "   0.00280605  0.00281373  0.00280698  0.00279185]\n",
      " [-0.03358726  0.00793204  0.00797569  0.00796892 -0.00200488 -0.02015193\n",
      "   0.00800008  0.00799052  0.00794528  0.00793154]\n",
      " [-0.03400219  0.00465846  0.00467866  0.00467468 -0.00339883  0.00468014\n",
      "   0.00468492  0.00469279  0.0046757   0.00465566]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.00373575  0.00372217  0.00374671  0.00373942 -0.0153423  -0.01455026\n",
      "   0.00375418  0.00374757  0.00372583  0.00372093]\n",
      " [-0.00893373  0.00285895  0.00287649  0.00287478  0.00285583 -0.01402237\n",
      "   0.00288973  0.00288041  0.00285979  0.00286011]\n",
      " [-0.00596497  0.00438355  0.00441029  0.00440387 -0.01047632 -0.01436399\n",
      "   0.00442114  0.00441455  0.00438926  0.00438262]\n",
      " [ 0.00149561  0.00148845  0.00149685  0.00149221 -0.01344651  0.00150174\n",
      "   0.00149355  0.00149773  0.00149425  0.0014861 ]\n",
      " [ 0.00317529  0.00316009  0.00317792  0.00316806 -0.02854789  0.00318831\n",
      "   0.00317092  0.0031798   0.00317241  0.0031551 ]\n",
      " [ 0.00484061  0.00496352  0.00499991  0.00499472 -0.00128541 -0.03846313\n",
      "   0.00502628  0.00499968  0.00495709  0.00496672]\n",
      " [-0.01159387  0.00365189  0.00367108  0.00366506 -0.01263267 -0.00142472\n",
      "   0.00367391  0.00367736  0.00366274  0.00364922]\n",
      " [-0.01638046  0.00181484  0.00182221  0.00182152  0.00180998  0.00182167\n",
      "   0.001826    0.00182864  0.00182148  0.00181412]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.00250718  0.00249596  0.00251067  0.00250366 -0.01926926 -0.00076467\n",
      "   0.00250796  0.00251191  0.00250374  0.00249284]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [-0.03370202  0.01273875  0.01281129  0.01279399 -0.02732586 -0.02848146\n",
      "   0.01283846  0.01282946  0.0127628   0.01273458]\n",
      " [-0.00408853  0.00226774  0.00228271  0.00228124  0.00226607 -0.01412419\n",
      "   0.00229449  0.00228471  0.0022667   0.00226907]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [-0.0335143   0.00810326  0.0081501   0.00814555  0.00809221 -0.03154087\n",
      "   0.00818408  0.00816421  0.00811032  0.00810544]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [-0.04519764  0.01453349  0.0146189   0.01460553 -0.00524321 -0.05171031\n",
      "   0.01466928  0.01464019  0.01454937  0.0145344 ]\n",
      " [-0.02384861  0.00282906  0.00284115  0.00284001  0.00282194  0.00115151\n",
      "   0.00284778  0.00285054  0.00283845  0.00282817]\n",
      " [ 0.00036111  0.00035938  0.00036141  0.00036029 -0.00324664  0.00036259\n",
      "   0.00036062  0.00036163  0.00036079  0.00035882]\n",
      " [-0.03025297  0.01133458  0.01140593  0.01139897  0.01132359 -0.06076515\n",
      "   0.01146063  0.01141956  0.01133497  0.0113399 ]\n",
      " [-0.01285146  0.00142385  0.00142964  0.00142909  0.00142004  0.00142921\n",
      "   0.00143261  0.00143468  0.00142906  0.00142328]\n",
      " [-0.00980452  0.00334838  0.00336915  0.00336713  0.0033449  -0.0170823\n",
      "   0.00338494  0.0033735   0.00334899  0.00334983]\n",
      " [ 0.00301279  0.00300312  0.00302397  0.00301938 -0.00692296 -0.01720156\n",
      "   0.00303468  0.00302426  0.00300283  0.0030035 ]\n",
      " [-0.01588316  0.00694326  0.00697883  0.00696341 -0.03986348  0.00699348\n",
      "   0.00697325  0.00698967  0.00696977  0.00693498]\n",
      " [-0.04825924  0.00848405  0.00852774  0.00852256  0.00468853 -0.01605042\n",
      "   0.00855438  0.00854763  0.00850119  0.00848357]\n",
      " [ 0.00505115  0.00503885  0.00507706  0.00507329  0.00503893 -0.04553715\n",
      "   0.00510935  0.00507632  0.00502848  0.00504373]\n",
      " [ 0.00012335  0.00012276  0.00012345  0.00012307 -0.00110898  0.00012385\n",
      "   0.00012318  0.00012352  0.00012324  0.00012256]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [-0.02530874  0.00506918  0.00509463  0.0050891  -0.0069696  -0.00333429\n",
      "   0.00510367  0.00510594  0.00508321  0.0050669 ]\n",
      " [ 0.00604023  0.00601687  0.00605539  0.0060422  -0.03073217 -0.01758178\n",
      "   0.00606236  0.00605722  0.0060263   0.00601337]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [-0.03493921  0.00387102  0.00388675  0.00388527  0.00386065  0.00388559\n",
      "   0.00389482  0.00390045  0.00388518  0.00386948]\n",
      " [ 0.00193547  0.00192621  0.00193708  0.00193107 -0.01740117  0.00194341\n",
      "   0.00193281  0.00193822  0.00193372  0.00192317]\n",
      " [ 0.00254169  0.00252952  0.0025438   0.0025359  -0.02285141  0.00255211\n",
      "   0.00253819  0.0025453   0.00253938  0.00252553]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [-0.02372753  0.00571843  0.00575145  0.00574825  0.00571061 -0.02218144\n",
      "   0.0057754   0.00576144  0.00572345  0.00571995]\n",
      " [-0.00801675  0.00861044  0.00866865  0.00866129  0.00215748 -0.05468709\n",
      "   0.0087111   0.00867397  0.00860603  0.00861488]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.00042332  0.00392765  0.00394925  0.00393796 -0.03195848  0.0039609\n",
      "   0.00394206  0.00395262  0.00394287  0.00392185]\n",
      " [-0.01808377  0.00488242  0.00490635  0.00489739 -0.02111628  0.00491418\n",
      "   0.00490539  0.004916    0.0049009   0.00487741]\n",
      " [-0.01236953  0.00226604  0.00227808  0.00227692  0.00226214 -0.00581904\n",
      "   0.00228629  0.00228313  0.00226972  0.00226624]\n",
      " [ 0.00185555  0.00561405  0.00564746  0.00563382 -0.03489647 -0.00638982\n",
      "   0.00564699  0.00565102  0.0056287   0.00560869]]\n",
      "grad_backprop: [[-0.01972031  0.00218487  0.00219375  0.00219291  0.00217902  0.0021931\n",
      "   0.00219831  0.00220148  0.00219286  0.002184  ]\n",
      " [ 0.00149651  0.00279612  0.00281169  0.00280332 -0.02394668  0.00282041\n",
      "   0.00280606  0.00281373  0.00280699  0.00279186]\n",
      " [-0.03358729  0.00793204  0.0079757   0.00796893 -0.00200488 -0.02015195\n",
      "   0.00800009  0.00799053  0.00794529  0.00793155]\n",
      " [-0.03400222  0.00465847  0.00467867  0.00467469 -0.00339883  0.00468015\n",
      "   0.00468493  0.00469279  0.0046757   0.00465567]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.00373576  0.00372217  0.00374671  0.00373942 -0.01534231 -0.01455027\n",
      "   0.00375418  0.00374757  0.00372583  0.00372094]\n",
      " [-0.00893374  0.00285896  0.0028765   0.00287479  0.00285583 -0.01402238\n",
      "   0.00288973  0.00288041  0.0028598   0.00286012]\n",
      " [-0.00596497  0.00438356  0.00441029  0.00440388 -0.01047633 -0.014364\n",
      "   0.00442114  0.00441455  0.00438926  0.00438262]\n",
      " [ 0.00149561  0.00148846  0.00149685  0.00149221 -0.01344652  0.00150174\n",
      "   0.00149355  0.00149774  0.00149426  0.0014861 ]\n",
      " [ 0.00317529  0.0031601   0.00317793  0.00316807 -0.02854792  0.00318831\n",
      "   0.00317092  0.0031798   0.00317241  0.0031551 ]\n",
      " [ 0.00484062  0.00496353  0.00499991  0.00499472 -0.00128541 -0.03846316\n",
      "   0.00502629  0.00499969  0.0049571   0.00496673]\n",
      " [-0.01159388  0.0036519   0.00367109  0.00366506 -0.01263269 -0.00142472\n",
      "   0.00367392  0.00367736  0.00366274  0.00364922]\n",
      " [-0.01638047  0.00181484  0.00182222  0.00182152  0.00180998  0.00182167\n",
      "   0.001826    0.00182864  0.00182148  0.00181412]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.00250719  0.00249596  0.00251068  0.00250366 -0.01926928 -0.00076467\n",
      "   0.00250796  0.00251191  0.00250374  0.00249285]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [-0.03370205  0.01273876  0.01281131  0.012794   -0.02732589 -0.02848149\n",
      "   0.01283848  0.01282947  0.01276281  0.0127346 ]\n",
      " [-0.00408854  0.00226774  0.00228271  0.00228124  0.00226607 -0.01412421\n",
      "   0.0022945   0.00228471  0.0022667   0.00226907]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [-0.03351433  0.00810327  0.00815011  0.00814556  0.00809222 -0.0315409\n",
      "   0.00818409  0.00816422  0.00811032  0.00810545]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [-0.04519768  0.0145335   0.01461891  0.01460555 -0.00524321 -0.05171036\n",
      "   0.0146693   0.0146402   0.01454938  0.01453442]\n",
      " [-0.02384864  0.00282907  0.00284115  0.00284001  0.00282194  0.00115151\n",
      "   0.00284778  0.00285055  0.00283845  0.00282817]\n",
      " [ 0.00036111  0.00035939  0.00036141  0.00036029 -0.00324664  0.00036259\n",
      "   0.00036062  0.00036163  0.00036079  0.00035882]\n",
      " [-0.030253    0.01133459  0.01140595  0.01139898  0.0113236  -0.06076521\n",
      "   0.01146064  0.01141957  0.01133498  0.01133991]\n",
      " [-0.01285147  0.00142385  0.00142964  0.00142909  0.00142004  0.00142921\n",
      "   0.00143261  0.00143468  0.00142906  0.00142329]\n",
      " [-0.00980453  0.00334838  0.00336916  0.00336713  0.0033449  -0.01708232\n",
      "   0.00338494  0.0033735   0.00334899  0.00334984]\n",
      " [ 0.00301279  0.00300312  0.00302397  0.00301938 -0.00692297 -0.01720158\n",
      "   0.00303469  0.00302426  0.00300283  0.00300351]\n",
      " [-0.01588317  0.00694326  0.00697884  0.00696342 -0.03986352  0.00699349\n",
      "   0.00697326  0.00698967  0.00696978  0.00693498]\n",
      " [-0.04825929  0.00848406  0.00852774  0.00852257  0.00468853 -0.01605044\n",
      "   0.00855439  0.00854764  0.0085012   0.00848358]\n",
      " [ 0.00505115  0.00503885  0.00507707  0.00507329  0.00503893 -0.04553719\n",
      "   0.00510936  0.00507632  0.00502848  0.00504374]\n",
      " [ 0.00012335  0.00012276  0.00012345  0.00012307 -0.00110898  0.00012385\n",
      "   0.00012318  0.00012352  0.00012324  0.00012256]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [-0.02530877  0.00506919  0.00509463  0.00508911 -0.0069696  -0.0033343\n",
      "   0.00510368  0.00510595  0.00508322  0.0050669 ]\n",
      " [ 0.00604024  0.00601688  0.0060554   0.00604221 -0.0307322  -0.0175818\n",
      "   0.00606237  0.00605722  0.00602631  0.00601337]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [-0.03493924  0.00387102  0.00388675  0.00388527  0.00386066  0.0038856\n",
      "   0.00389482  0.00390045  0.00388518  0.00386948]\n",
      " [ 0.00193548  0.00192622  0.00193708  0.00193107 -0.01740119  0.00194341\n",
      "   0.00193281  0.00193823  0.00193372  0.00192317]\n",
      " [ 0.00254169  0.00252953  0.0025438   0.00253591 -0.02285143  0.00255211\n",
      "   0.00253819  0.0025453   0.00253938  0.00252553]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [-0.02372755  0.00571843  0.00575146  0.00574825  0.00571061 -0.02218147\n",
      "   0.0057754   0.00576145  0.00572345  0.00571996]\n",
      " [-0.00801676  0.00861045  0.00866866  0.00866129  0.00215748 -0.05468714\n",
      "   0.00871111  0.00867398  0.00860604  0.00861489]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.00042332  0.00392765  0.00394926  0.00393796 -0.03195851  0.0039609\n",
      "   0.00394206  0.00395262  0.00394287  0.00392186]\n",
      " [-0.01808378  0.00488243  0.00490636  0.0048974  -0.0211163   0.00491418\n",
      "   0.00490539  0.004916    0.00490091  0.00487742]\n",
      " [-0.01236954  0.00226604  0.00227809  0.00227692  0.00226214 -0.00581904\n",
      "   0.0022863   0.00228313  0.00226972  0.00226624]\n",
      " [ 0.00185555  0.00561406  0.00564747  0.00563383 -0.0348965  -0.00638982\n",
      "   0.005647    0.00565102  0.0056287   0.00560869]]\n",
      "b2:1.402139722744833e-07\n",
      "grad_numerical: [-0.23330116  0.09965875  0.10023306  0.10009185 -0.23371849 -0.23323765\n",
      "  0.10044155  0.10036548  0.09984142  0.09962521]\n",
      "grad_backprop: [-0.23330139  0.09965885  0.10023316  0.10009195 -0.23371873 -0.23323788\n",
      "  0.10044165  0.10036558  0.09984152  0.09962531]\n"
     ]
    }
   ],
   "source": [
    "# 误差反向传播法的梯度确认: 微分法 和 解析法 求差值；\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 读入数据\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label = True)\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "x_batch = x_train[:3]\n",
    "\n",
    "t_batch = t_train[:3]\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 求各个权重的绝对误差的平均值\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key + \":\" + str(diff))\n",
    "    print('grad_numerical:',grad_numerical[key])\n",
    "    print('grad_backprop:', grad_backprop[key])\n",
    "\n",
    "# 从这个结果可以看出，通过数值微分和误差反向传播法求出的梯度的差非常小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d0bdc04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T12:24:35.035392Z",
     "start_time": "2022-10-21T12:24:09.152016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05608333333333333 0.0538\n",
      "0.9048833333333334 0.9068\n",
      "0.92255 0.9233\n",
      "0.9353666666666667 0.9334\n",
      "0.9452166666666667 0.9441\n",
      "0.9520666666666666 0.9502\n",
      "0.9578 0.9533\n",
      "0.9617666666666667 0.9576\n",
      "0.9637166666666667 0.959\n",
      "0.96805 0.9627\n",
      "0.9703 0.9653\n",
      "0.97155 0.9647\n",
      "0.9744666666666667 0.9674\n",
      "0.9746666666666667 0.9675\n",
      "0.9762833333333333 0.9672\n",
      "0.97805 0.9703\n",
      "0.9792 0.9692\n"
     ]
    }
   ],
   "source": [
    "# 使用误差反向传播法的学习\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "# 读入数据\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 通过误差反向传播法求梯度\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    # 更新\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a72cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba267d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6af642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a58b0e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba251e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
