{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e8e7930",
   "metadata": {},
   "source": [
    "# 数据驱动的基本思路与损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0b2ce8",
   "metadata": {},
   "source": [
    "## 数据驱动的基本思路\n",
    "\n",
    "我们先用一个不严谨的例子，对数据学习有一点感性上的认识\n",
    "\n",
    "假设有Y = AX + B\n",
    "\n",
    "我们已知 X, Y 的值，求A和B，思路是：\n",
    "\n",
    "### 第一步：训练\n",
    "随机给一个 $A_1$, $B_1$，计算出 $Y_{p1}$，得到与正确预测的差值 |$Y$ - $Y_{p1}$|， p在这里是predict的缩写。\n",
    "\n",
    "调整权重为 $A_2$, $B_2$，得到 $Y_{p2}$，并有 |$Y$ - $Y_{p2}$| < |$Y$ - $Y_{p1}$|\n",
    "\n",
    "重复这个过程：直到预测的 $Y_{pn}$ 与已知Y的差距越来越小，这时的$A_{n}$ ，$B_{n}$ 就是我们求得的参数\n",
    "\n",
    "### 第二步：测试 $Y=A_nX + B_n$\n",
    "\n",
    "再来一组已知数据 $X_{2}$ ，$Y_{2}$，测试|$Y_{2} - Y_{p}$| 以判断函数的准确性\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067693e1",
   "metadata": {},
   "source": [
    "### 基本概念的引入\n",
    "\n",
    "可见在这个过程中，我们只需要知道X，Y的数据，而二者之间的关系，则由机器自动推算出来。其中：\n",
    "\n",
    "$X_1$, $Y_1$ 为 **训练数据**\n",
    "\n",
    "$X_2$， $Y_2$ 为 **测试数据**\n",
    "\n",
    "|$Y_{1} - Y_{p}$| 为 **损失函数**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c1d610",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T08:04:00.588364Z",
     "start_time": "2022-10-18T08:04:00.567944Z"
    }
   },
   "source": [
    "### 传统思维方式，机器学习 和 神经网络（深度学习）的区别\n",
    "\n",
    "如图：\n",
    "<img src=\"img/0402.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "神经网络或深度学习则比以往的机器学习方法更能避免人为介入。与待处理的问题无关，神经网络可以将数据直接作为原始数据，进行“端对端”的学习。\n",
    "\n",
    "#### 传统机器学习步骤：\n",
    "人为设计一些特征值，再通过机器学习SVM，KNN 来实现；\n",
    "\n",
    "#### 神经网络学习步骤：\n",
    "直接学习图像本身\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f8a872",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "\n",
    "- 神经网络的学习通过某个指标表示现在的状态。然后，以这个指标为基准，寻找最优权重参数。\n",
    "\n",
    "- 神经网络以某个指标为线索寻找最优权重参数\n",
    "\n",
    "- 神经网络的学习中所用的指标称为损失函数（loss function）。\n",
    "\n",
    "- 损失函数是表示神经网络性能的“恶劣程度”的指标，即当前的神经网络对监督数据在多大程度上不拟合，在多大程度上不一致\n",
    "\n",
    "- 这个损失函数可以使用任意函数，但一般用均方误差和交叉熵误差等。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccedb602",
   "metadata": {},
   "source": [
    "### 均方误差MSE（mean squared error）\n",
    "\n",
    "#### 公式 ：$E = \\frac{1}{2}\\sum\\limits_k(y_k - t_k)^2 $\n",
    "$y_k$是表示神经网络的输出，$t_k$表示监督数据，k表示数据的维数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d59c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在节手写数字识别的例子中，yk、tk是由如下10个元素构成的数据。\n",
    "\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] # 0~9 数字的概率\n",
    "\n",
    "# 数组元素的索引从第一个开始依次对应数字“0”“1”“2”…… \n",
    "# 这里，神经网络的输出y是softmax函数的输出。\n",
    "# 由于softmax函数的输出可以理解为概率，\n",
    "# 因此上例表示“0”的概率是0.1，“1”的概率是0.05，“2”的概率是0.6等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1c725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "# t是监督数据，将正确解标签设为1，其他均设为0。\n",
    "# 这里，标签“2”为1，表示正确解是“2”。\n",
    "# 将正确解标签表示为1，其他标签表示为0的表示方法称为one-hot表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5fbec9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T02:34:23.190248Z",
     "start_time": "2022-10-18T02:34:23.184510Z"
    }
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "09e8ab5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T08:25:58.262538Z",
     "start_time": "2022-10-18T08:25:58.241296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "print(np.argmax(y)) # 返回y数组中，最大元素所对应的角标\n",
    "\n",
    "mean_squared_error(np.array(y), np.array(t))\n",
    "\n",
    "# 这里最后输出的是error，越小说明误差越小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a493c5bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T08:27:09.288856Z",
     "start_time": "2022-10-18T08:27:09.257462Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # 例2：t不变，修改y：“7”的概率最高的情况（0.6）,t 的正确解还是2\n",
    "y2 = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "mean_squared_error(np.array(y2), np.array(t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848f075a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T02:40:05.324535Z",
     "start_time": "2022-10-18T02:40:05.317834Z"
    }
   },
   "source": [
    "### 交叉熵误差(cross entropy error)\n",
    "\n",
    "#### 公式： $E = -\\sum\\limits_k t_k log y_k $\n",
    "\n",
    "- log表示以e为底数的自然对数（$log_e$）, $y_k$是神经网络的输出，$t_k$是正确解标签。\n",
    "\n",
    "- $t_k$中只有正确解标签的索引为1，其他均为0（one-hot表示）。\n",
    "\n",
    "- 因此，交叉熵公式实际上只计算对应正确解标签的输出的自然对数。\n",
    "\n",
    "比如，假设正确解标签的索引是“2”，与之对应的神经网络的输出是0.6，则交叉熵误差是−log 0.6 = 0.51；\n",
    "若“2”对应的输出是0.1，则交叉熵误差为−log 0.1 = 2.30。也就是说，交叉熵误差的值是由正确解标签所对应的输出结果决定的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cc94bdb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T09:14:35.204866Z",
     "start_time": "2022-10-18T09:14:35.041141Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-105-102c02e36cda>:7: RuntimeWarning: divide by zero encountered in log\n",
      "  y = -np.log(x)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 看一下-log(x) 函数\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "x = np.arange(0, 1, 0.001)\n",
    "y = -np.log(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.ylim(0, 5.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e14032",
   "metadata": {},
   "source": [
    "对应到公式中：$E = -\\sum\\limits_k t_k log y_k $\n",
    "\n",
    "正确解标签对应的输出越大，公式中的值越接近0（也就是误差越小）。以至于当输出为1时，交叉熵误差E为0。\n",
    "\n",
    "如果正确解标签对应的输出较小，则公式的值较大，则误差E越大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "dd27ca62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T09:22:27.227152Z",
     "start_time": "2022-10-18T09:22:27.216197Z"
    }
   },
   "outputs": [],
   "source": [
    "# 代码实现交叉熵误差\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "\n",
    "# 函数内部在计算np.log时，加上了一个极小值delta。\n",
    "# 这是因为，当出现np.log(0)时，np.log(0)会变为负无限大的-inf，这样一来就会导致后续计算无法进行。\n",
    "# 作为保护性对策，添加一个微小值可以防止负无限大的发生。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ca3636a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T10:58:28.690479Z",
     "start_time": "2022-10-18T10:58:28.679980Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第一个例子，正确解标签对应的输出为0.6，此时的交叉熵误差大约为0.51\n",
    "\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))\n",
    "\n",
    "# 误差为0.51..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48a3440e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T03:11:48.923994Z",
     "start_time": "2022-10-18T03:11:48.897708Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302584092994546"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第二个例子，正确解标签对应的输出为0.1的低值，此时的交叉熵误差大约为2.3\n",
    "\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))\n",
    "\n",
    "# 误差为 2.3.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c6e81728",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T11:04:46.844298Z",
     "start_time": "2022-10-18T11:04:46.831304Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -2.30258409  -2.99573027  -0.51082546 -16.11809565  -2.99573027\n",
      "  -2.30258409 -16.11809565  -2.30258409 -16.11809565 -16.11809565]\n",
      "[-0.         -0.         -0.51082546 -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.        ]\n",
      "0.510825457099338\n"
     ]
    }
   ],
   "source": [
    "# 详细解读：-np.sum(t * np.log(y + delta))\n",
    "\n",
    "delta = 1e-7\n",
    "print(np.log(np.array(y) + delta))\n",
    "\n",
    "print(np.array(t)*np.log(np.array(y) + delta))\n",
    "\n",
    "print(-np.sum(np.array(t)*np.log(np.array(y) + delta)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9ae10a",
   "metadata": {},
   "source": [
    "### mini-batch学习\n",
    "\n",
    "神经网络的学习也是从训练数据中选出一批数据（称为mini-batch,小批量），然后对每个mini-batch进行学习。\n",
    "\n",
    "比如，从60000个训练数据中随机\n",
    "选择100笔，再用这100笔数据进行学习。这种学习方式称为mini-batch学习。\n",
    "\n",
    "#### 公式：$E = \\frac{1}{N}\\sum\\limits_{n} \\sum\\limits_{k} t_{nk} log y_{nk} $\n",
    "\n",
    "这里,假设数据有N个，$t_{nk}$表示第n个数据的第k个元素的值（$y_{nk}$是神经网络的输出，$t_{nk}$是监督数据）。\n",
    "\n",
    "其实只是把求单个数据的损失函数扩大到了N份数据，最后除以N进行正规化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8971d7d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T10:40:32.216490Z",
     "start_time": "2022-10-18T10:40:32.194366Z"
    }
   },
   "outputs": [],
   "source": [
    "# mini-batch版交叉熵误差的实现\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size\n",
    "\n",
    "# 非batch：-np.sum(t * np.log(y + 1e-7))\n",
    "\n",
    "# 其中，1e-7 为极小值，如果不加这个值，当y=0的时候，log返回的结果是无穷大。会报错！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f637d8d",
   "metadata": {},
   "source": [
    "这里，y是神经网络的输出，t是监督数据。\n",
    "\n",
    "y的维度为1时，即求单个数据的交叉熵误差时，需要改变数据的形状。\n",
    "\n",
    "并且，当输入为mini-batch时，要用batch的个数进行正规化，计算单个数据的平均交叉熵误差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4574a4b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T10:45:30.842150Z",
     "start_time": "2022-10-18T10:45:30.801834Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果y为1维：\n",
    "\n",
    "y1 = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])\n",
    "t1 = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "cross_entropy_error(y1, t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "024f84ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T10:52:27.453108Z",
     "start_time": "2022-10-18T10:52:27.431546Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.406704775046942"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果y为多维\n",
    "y2 = np.array([[0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0],\n",
    "               [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0],\n",
    "               [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0],\n",
    "               [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]])\n",
    "# 这里把y2中的值稍加改动；\n",
    "\n",
    "t2 = np.array([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "               [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "               [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "               [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]])\n",
    "\n",
    "cross_entropy_error(y2, t2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "335b1fa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T10:56:43.494401Z",
     "start_time": "2022-10-18T10:56:43.483484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -2.30258409  -2.99573027  -0.51082546 -16.11809565  -2.99573027\n",
      "   -2.30258409 -16.11809565  -2.30258409 -16.11809565 -16.11809565]\n",
      " [ -2.30258409  -2.99573027  -0.51082546 -16.11809565  -2.99573027\n",
      "   -2.30258409 -16.11809565  -2.30258409 -16.11809565 -16.11809565]\n",
      " [ -2.30258409  -2.99573027  -2.30258409 -16.11809565  -2.99573027\n",
      "   -2.30258409 -16.11809565  -0.51082546 -16.11809565 -16.11809565]\n",
      " [ -2.30258409  -2.99573027  -2.30258409 -16.11809565  -2.99573027\n",
      "   -2.30258409 -16.11809565  -0.51082546 -16.11809565 -16.11809565]]\n",
      "[[-0.         -0.         -0.51082546 -0.         -0.         -0.\n",
      "  -0.         -0.         -0.         -0.        ]\n",
      " [-0.         -0.         -0.51082546 -0.         -0.         -0.\n",
      "  -0.         -0.         -0.         -0.        ]\n",
      " [-0.         -0.         -2.30258409 -0.         -0.         -0.\n",
      "  -0.         -0.         -0.         -0.        ]\n",
      " [-0.         -0.         -2.30258409 -0.         -0.         -0.\n",
      "  -0.         -0.         -0.         -0.        ]]\n",
      "1.406704775046942\n"
     ]
    }
   ],
   "source": [
    "# 详细解读： -np.sum(t * np.log(y + 1e-7)) / batch_size\n",
    "batch_size = 4\n",
    "print(np.log(y2 + 1e-7))\n",
    "\n",
    "print(t2 * np.log(y2 + 1e-7))\n",
    "\n",
    "print(-np.sum(t2 * np.log(y2 + 1e-7))/ batch_size)\n",
    "# -(-0.51082546-0.51082546-2.30258409-2.30258409)/4 = 5.6268191/4 = 1.406704775046942"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c994a80f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T10:49:10.153350Z",
     "start_time": "2022-10-18T10:49:10.141180Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.406704775046942"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 另，当监督数据是标签形式（非one-hot表示，而是像“2”“7”这样的标签）时，优化实现：\n",
    "\n",
    "def cross_entropy_error2(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "# 结果\n",
    "y3 = np.array([[0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0],\n",
    "               [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0],\n",
    "               [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0],\n",
    "               [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]])\n",
    "# 这里把y2中的值稍加改动；\n",
    "t3 = np.array([2,2,2,2])\n",
    "\n",
    "# 最终输出：\n",
    "cross_entropy_error2(y3,t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da734fd2",
   "metadata": {},
   "source": [
    "### 回到MNIST数据集 的例子，我们取数来看下\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "62d4b1a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T11:14:56.635698Z",
     "start_time": "2022-10-18T11:14:56.188137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# 回到MNIST数据集 的例子，我们取数来看下，\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "print(x_train.shape) # (60000, 784)\n",
    "print(t_train.shape) # (60000, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03c3ec0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T11:14:59.362374Z",
     "start_time": "2022-10-18T11:14:59.351301Z"
    }
   },
   "source": [
    "读入MNIST数据后，训练数据有60000个，输入数据是784维 （28 × 28）的图像数据，监督数据是10维的数据。因此，上面的x_train、t_\n",
    "train的形状分别是(60000, 784)和(60000, 10)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "47fc0485",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T11:16:03.761282Z",
     "start_time": "2022-10-18T11:16:03.752633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "[43568 39237 39090 14477  3746 51454 25833 30525 14245 31074]\n"
     ]
    }
   ],
   "source": [
    "# 从这个训练数据中随机抽取10笔数据\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size) # 从0到59999之间随机选择10个数字\n",
    "print(train_size)\n",
    "print(batch_mask)\n",
    "\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "# print(x_batch)\n",
    "# print(t_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d851d80",
   "metadata": {},
   "source": [
    "之后，我们只需指定这些随机选出的索引，取出mini-batch，然后使用这个mini-batch计算损失函数即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24492aaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T11:18:24.646746Z",
     "start_time": "2022-10-18T11:18:24.630650Z"
    }
   },
   "source": [
    "### 为何要设定损失函数\n",
    "\n",
    "在进行神经网络的学习时，不能将识别精度作为指标。因为**如果以识别精度为指标，则参数的导数在绝大多数地方都会变为0。**\n",
    "\n",
    "#### 为什么用识别精度作为指标时，参数的导数在绝大多数地方都会变成0？\n",
    "答案：假设某个神经网络正确识别出了100笔训练数据中的32笔，此时识别精度为32 %。如果以识别精度为指标，即使稍微改变权重参数的值，识别精度也仍将保持在32 %，不会出现变化。也就是说，仅仅微调参数，是无法改善识别精度的。即便识别精度有所改善，它的值也不会像32.0123 ... %这样连续变化，而是变为33 %、34 %这样的不连续的、离散的值。而如果把损失函数作为指标，则当前损失函数的值可以表示为0.92543 ... 这样的值。并且，如果稍微改变一下参数的值，对应的损失函数也会像0.93432 ... 这样发生连续性的变化。\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"img/0404.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "识别精度对微小的参数变化基本上没有什么反应，即便有反应，它的值也是不连续地、突然地变化。\n",
    "作为激活函数的阶跃函数也有同样的情况。出于相同的原因，如果使用阶跃函数作为激活函数，神经网络的学习将无法进行。\n",
    "\n",
    "如上图所示，阶跃函数的导数在绝大多数地方（除了0以外的地方）均为0。\n",
    "也就是说，如果使用了阶跃函数，那么即便将损失函数作为指标，参数的微小变化也会被阶跃函数抹杀，导致损失函数的值不会产生任何变化。\n",
    "\n",
    "而sigmoid函数，不仅函数的输出（竖轴的值）是连续变化的，曲线的斜率（导数）也是连续变化的。也就是说，sigmoid函数的导数在任何地方都不为0。这对神经网络的学习非常重要。得益于这个斜率不会为0的性质，神经网络的学习得以正确进行。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acbee6f",
   "metadata": {},
   "source": [
    "# 数值微分\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0398e885",
   "metadata": {},
   "source": [
    "## 导数\n",
    "导数就是表示某个瞬间的变化量。它可以定义成下面的式子。\n",
    "\n",
    "$$\\frac{df(x)}{dx}= \\lim\\limits_{h\\rightarrow0}\\frac{f(x+h)-f(x)}{h} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0b33afa0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T12:27:21.799003Z",
     "start_time": "2022-10-18T12:27:21.789058Z"
    }
   },
   "outputs": [],
   "source": [
    "# 中心差分实现数值微分\n",
    "\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4 # 0.0001,使用过小的值会造成计算机出现计算上的问题，发生“舍入误差”，反而舍入为0.0\n",
    "    return (f(x+h) - f(x-h)) / (2*h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f6acc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T12:25:12.309252Z",
     "start_time": "2022-10-18T12:25:12.280585Z"
    }
   },
   "source": [
    "利用微小的差分求导数的过程称为数值微分，如以上的 numerical_diff 计算结果；--- 近似导数\n",
    "\n",
    "而基于数学式的推导求导数的过程，则用“解析性”（analytic）一词，称为“解析性求解”或者“解析性求导”。--- “真的导数”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e5eabda3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T12:26:16.024862Z",
     "start_time": "2022-10-18T12:26:15.794386Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 例子\n",
    "\n",
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "x = np.arange(0.0, 20.0, 0.1) # 以0.1为单位，从0到20的数组x\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "beedde99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T12:27:24.590988Z",
     "start_time": "2022-10-18T12:27:24.580900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1999999999990898\n",
      "0.2999999999986347\n"
     ]
    }
   ],
   "source": [
    "# 计算导数\n",
    "print(numerical_diff(function_1, 5))\n",
    "print(numerical_diff(function_1, 10))\n",
    "\n",
    "#  在x = 5 和 x = 10处，通过解析解得到的“真的导数”分别为0.2和0.3。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b09fde",
   "metadata": {},
   "source": [
    "## 偏导数\n",
    "\n",
    "我们把多个变量的函数的导数称为偏导数。\n",
    "\n",
    "不过，偏导数需要将多个变量中的某一个变量定为目标变量，并将其他变量固定为某个值。\n",
    "\n",
    "在下例的代码中，为了将目标变量以外的变量固定到某些特定的值上，我们定义了新函数。\n",
    "\n",
    "然后，对新定义的函数应用了之前的求数值微分的函数，得到偏导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38d5525",
   "metadata": {},
   "outputs": [],
   "source": [
    "求某个变量x0偏导的时候，实际上和x1没有任何关系。偏导只是代表函数在某个变量上的变化情况（切线斜率）；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d620f825",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T13:05:42.534971Z",
     "start_time": "2022-10-18T13:05:42.529985Z"
    }
   },
   "outputs": [],
   "source": [
    "# 多元方程式\n",
    "\n",
    "# 有两个变量x\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2 # 或者可以写成：return np.sum(x**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d850533a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T12:45:12.780549Z",
     "start_time": "2022-10-18T12:45:12.769720Z"
    }
   },
   "source": [
    "问题1：求x0 = 3, x1 = 4时，关于x0的偏导数 ${{\\partial}f}\\over{{\\partial}{x_0}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "70c1cfcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T12:42:17.499464Z",
     "start_time": "2022-10-18T12:42:17.486219Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_tmp1(x0):\n",
    "    return x0*x0 + 4.0**2.0\n",
    "\n",
    "numerical_diff(function_tmp1, 3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fa3dfb",
   "metadata": {},
   "source": [
    "问题2：求x0 = 3, x1 = 4时，关于x1的偏导数 ${{\\partial}f}\\over{{\\partial}{x_1}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1df591ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T12:49:35.020231Z",
     "start_time": "2022-10-18T12:49:35.000858Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_tmp2(x1):\n",
    "    return 3.0**2.0 + x1*x1\n",
    "\n",
    "numerical_diff(function_tmp2, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc77c10b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T12:51:06.450058Z",
     "start_time": "2022-10-18T12:51:06.444702Z"
    }
   },
   "source": [
    "# 梯度\n",
    "\n",
    "在刚才的例子中，我们按变量分别计算了x0和x1的偏导数。\n",
    "\n",
    "如果将两个偏导汇总形成一个值，称为梯度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "be1a1bfd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T13:05:35.642639Z",
     "start_time": "2022-10-18T13:05:35.622830Z"
    }
   },
   "outputs": [],
   "source": [
    "# 梯度实现的代码\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x) # 生成和x形状相同的数组[0,0]\n",
    "    \n",
    "    for idx in range(x.size): # 如X[3, 4], idx = 0, 1\n",
    "        tmp_val = x[idx] # tmp_val = X[0] = 3\n",
    "        \n",
    "        # f(x+h)的计算\n",
    "        x[idx] = tmp_val + h # X[0]=3+h, X变为[3+h, 4]\n",
    "        fxh1 = f(x) # 算 [3+h, 4]对应的f\n",
    "        \n",
    "        # f(x-h)的计算\n",
    "        x[idx] = tmp_val - h # X为[3-h, 4]\n",
    "        fxh2 = f(x) # 算 [3-h, 4] 对应的f\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h) # 得到梯度\n",
    "        x[idx] = tmp_val # 还原X为[3, 4]\n",
    "    \n",
    "    return grad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b6bfdea5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T13:09:26.452071Z",
     "start_time": "2022-10-18T13:09:26.441354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 8.]\n",
      "[0. 4.]\n",
      "[6. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 调用\n",
    "print(numerical_gradient(function_2, np.array([3.0, 4.0])))\n",
    "print(numerical_gradient(function_2, np.array([0.0, 2.0])))\n",
    "print(numerical_gradient(function_2, np.array([3.0, 0.0])))\n",
    "\n",
    "# 梯度指示的方向是各点处的函数值减小最多的方向 A。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d3f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接整理成笔记的效率有点慢，还是先以书本和代码理解为主。\n",
    "# 后续复习的时候再整理笔记。\n",
    "# 先在书中把关键信息标记下来。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "10f68679",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-19T02:53:30.619594Z",
     "start_time": "2022-10-19T02:53:30.546209Z"
    }
   },
   "outputs": [],
   "source": [
    "# 梯度下降法实现\n",
    "\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    return x\n",
    "\n",
    "# 参数f是要进行最优化的函数\n",
    "# init_x是初始值，\n",
    "# lr是学习率learning rate，\n",
    "# step_num是梯度法的重复次数\n",
    "\n",
    "# numerical_gradient(f,x)会求函数的梯度，用该梯度乘以学习率得到的值进行更新操作，\n",
    "#由step_num指定重复的次数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "de947562",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-19T02:53:36.655421Z",
     "start_time": "2022-10-19T02:53:36.623749Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 问题：请用梯度法求 以下的最小值。\n",
    "\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)\n",
    "\n",
    "# 可见最终输出是两个极小值，趋近于(0, 0)，实际最终的结果就是(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d1c7a6b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-19T02:57:48.525624Z",
     "start_time": "2022-10-19T02:57:48.297614Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 所有代码 + 绘图\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x) # 生成和x形状相同的数组[0,0]\n",
    "    \n",
    "    for idx in range(x.size): # 如X[3, 4], idx = 0, 1\n",
    "        tmp_val = x[idx] # tmp_val = X[0] = 3\n",
    "        \n",
    "        # f(x+h)的计算\n",
    "        x[idx] = tmp_val + h # X[0]=3+h, X变为[3+h, 4]\n",
    "        fxh1 = f(x) # 算 [3+h, 4]对应的f\n",
    "        \n",
    "        # f(x-h)的计算\n",
    "        x[idx] = tmp_val - h # X为[3-h, 4]\n",
    "        fxh2 = f(x) # 算 [3-h, 4] 对应的f\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h) # 得到梯度\n",
    "        x[idx] = tmp_val # 还原X为[3, 4]\n",
    "    \n",
    "    return grad\n",
    "    \n",
    "\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    x_history = []\n",
    "\n",
    "    for i in range(step_num):\n",
    "        x_history.append( x.copy() )\n",
    "\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "    return x, np.array(x_history)\n",
    "\n",
    "\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])    \n",
    "\n",
    "lr = 0.1\n",
    "step_num = 20\n",
    "x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)\n",
    "\n",
    "plt.plot( [-5, 5], [0,0], '--b')\n",
    "plt.plot( [0,0], [-5, 5], '--b')\n",
    "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
    "\n",
    "plt.xlim(-3.5, 3.5)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e663018b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-19T04:55:09.758771Z",
     "start_time": "2022-10-19T04:55:09.730572Z"
    }
   },
   "outputs": [],
   "source": [
    "# 简单的神经网络为例，来实现求梯度的代码\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class simpleNet:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 用高斯分布进行初始化\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f425d960",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-19T04:56:24.435382Z",
     "start_time": "2022-10-19T04:56:24.417553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.59123297  1.71024669 -0.92667374]\n",
      " [-0.60003028  1.83213168  1.13900138]]\n",
      "[-0.89476703  2.67506653  0.469097  ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.3355081321353173"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simpleNet 简单试用\n",
    "net = simpleNet()\n",
    "print(net.W) # 权重参数\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "\n",
    "np.argmax(p) # 最大值的索引\n",
    "t = np.array([0, 0, 1]) # 正确解标签\n",
    "net.loss(x, t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5b580852",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-19T05:00:41.447467Z",
     "start_time": "2022-10-19T05:00:41.436947Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0148434   0.52709932 -0.54194272]\n",
      " [ 0.0222651   0.79064898 -0.81291408]]\n"
     ]
    }
   ],
   "source": [
    "# 求一次梯度；\n",
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "2e870bf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-19T05:10:29.968000Z",
     "start_time": "2022-10-19T05:10:29.959401Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0148434   0.52709932 -0.54194272]\n",
      " [ 0.0222651   0.79064898 -0.81291408]]\n"
     ]
    }
   ],
   "source": [
    "# 使用匿名函数\n",
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "18d1c825",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-19T05:18:35.428110Z",
     "start_time": "2022-10-19T05:18:35.372004Z"
    }
   },
   "outputs": [],
   "source": [
    "# 　2层神经网络的类\n",
    "\n",
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 为了导入父目录的文件而进行的设定\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 初始化权重\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x:输入数据, t:监督数据\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x:输入数据, t:监督数据\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "16f35f76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-19T05:19:08.683261Z",
     "start_time": "2022-10-19T05:19:08.666393Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "# 调用例子\n",
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "print(net.params['W1'].shape) # (784, 100)\n",
    "print(net.params['b1'].shape) # (100,)\n",
    "print(net.params['W2'].shape) # (100, 10)\n",
    "print(net.params['b2'].shape) # (10,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8cb7e7b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-19T05:21:49.631262Z",
     "start_time": "2022-10-19T05:21:49.620221Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.34514322e-02,  1.09087085e-02, -2.01157826e-03,  8.00129735e-03,\n",
       "       -5.22080220e-03,  1.11736478e-04, -3.74985749e-03, -6.50379196e-03,\n",
       "        4.85889599e-03, -1.29128624e-03,  1.90275867e-02, -1.31734069e-02,\n",
       "        1.69986161e-02, -1.25314489e-02,  1.46652226e-03, -9.01381680e-03,\n",
       "        2.35743488e-03, -7.41052520e-03, -3.97676975e-03,  3.10187835e-04,\n",
       "       -2.49544036e-03, -6.12340542e-03, -6.36124288e-03,  6.17222058e-03,\n",
       "        7.77677796e-03,  5.62910708e-03, -8.62246295e-03, -1.04294822e-02,\n",
       "       -7.39197993e-03,  6.75469596e-03, -1.84672666e-02, -3.08378890e-04,\n",
       "       -4.15515706e-04,  7.90457842e-03,  7.84685725e-03, -4.39315539e-04,\n",
       "        7.08390930e-03,  2.18714532e-03,  2.16632191e-03,  6.75673218e-04,\n",
       "       -7.05394555e-03, -6.47511114e-03,  1.52287362e-02,  8.12595770e-03,\n",
       "       -1.00245903e-02,  1.78137864e-03, -1.28585201e-02, -1.53259152e-02,\n",
       "        6.51237629e-03, -7.16438194e-04, -8.57813316e-03, -1.78967634e-03,\n",
       "       -8.24522478e-03,  4.82481043e-03,  5.64404454e-03, -1.83411032e-02,\n",
       "       -9.36998390e-03, -7.20083055e-03, -1.79324826e-04, -1.32390849e-02,\n",
       "       -1.32998556e-02, -1.00433891e-02, -8.32396416e-03, -1.93410963e-03,\n",
       "        8.52834772e-03,  3.17127254e-05, -2.03119056e-02,  1.17346908e-02,\n",
       "       -1.74113978e-02,  1.12098847e-02, -6.97655496e-03,  3.37220803e-03,\n",
       "       -1.42709162e-02,  1.10434411e-02, -9.90771040e-04, -7.19665258e-03,\n",
       "       -7.05932223e-03,  5.94754373e-03,  5.92820829e-03, -1.50151908e-02,\n",
       "       -2.07318479e-03, -1.20128638e-02,  2.19014779e-02,  1.89770809e-02,\n",
       "       -1.68895350e-02, -1.09216595e-02,  6.35377768e-03, -8.68748919e-03,\n",
       "        8.59494792e-03, -5.12296759e-03,  1.00816490e-02,  7.71511540e-03,\n",
       "        6.90299477e-03, -6.26452175e-03,  7.06654863e-03, -2.06174792e-02,\n",
       "       -3.76297845e-03, -1.51081503e-02,  3.80853620e-03, -2.92645736e-03])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.params['W1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8defc6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11596a67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae716df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0397e08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6547f42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6f9c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
